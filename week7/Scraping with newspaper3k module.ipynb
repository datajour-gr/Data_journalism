{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping με το newspaper3k**"
      ],
      "metadata": {
        "id": "7pzpozYbs3V9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping, είναι η διαδικασία με την οποία συλλέγουμε περιεχόμενο από ιστοσελίδες στο διαδίκτυο.\n",
        "\n",
        "Με όρους προγραμματισμού, η διαδικασία είναι η εξής:\n",
        "- κάνουμε ένα αίτημα στο server της ιστοσελίδας: ορίζουμε το url της ιστοσελίδας και ζητάμε το περιεχόμενό της (δηλαδή τον html κώδικά της)\n",
        "- κατεβάζουμε τον html κώδικα (στον οποίο το περιεχόμενο είναι οργανωμένο μέσα σε html tags)\n",
        "- παίρνουμε το περιεχόμενο από τα tags που μας ενδιαφέρουν και το αποθηκεύουμε σε αντίστοιχες μεταβλητές (π.χ. αν πρόκειται για δημοσιογραφικό άρθρο, παίρνουμε τον τίτλο του, το κείμενο, τον συγγραφέα, την ημερομηνία δημοσίευσής του κλπ.)\n",
        "\n",
        "Στη συνέχεια, μπορούμε να οργανώσουμε όλες τις πληροφορίες που συλλέξαμε σε ένα dataframe για να τις επεξεργαστούμε και να τις αναλύσουμε.\n",
        "\n",
        "Το **newspaper** είναι ένα πακέτο για την python, με το οποίο μπορούμε με εύκολο τρόπο να συλλέξουμε περιεχόμενο από ειδησεογραφικές ιστοσελίδες.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Sources:\n",
        "- Documentation: https://newspaper.readthedocs.io/en/latest/\n",
        "- Scraping websites with Newspaper3k in Python: https://www.geeksforgeeks.org/scraping-websites-with-newspaper3k-in-python/\n",
        "- Scrape and Summarize News Articles in 5 Lines of Python Code: https://towardsdatascience.com/scrape-and-summarize-news-articles-in-5-lines-of-python-code-175f0e5c7dfc"
      ],
      "metadata": {
        "id": "-qPvOZvmtFaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Εγκατάσταση πακέτου**"
      ],
      "metadata": {
        "id": "h9RcAOOdwd-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ξέρουμε ότι για να μπορούμε να χρησιμοποιήσουμε τις συναρτήσεις ενός πακέτου πρέπει πρώτα να το εισάγουμε. "
      ],
      "metadata": {
        "id": "xyyVAHbuwpW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper"
      ],
      "metadata": {
        "id": "N3wpXcVFxTGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η εκτέλεση του παραπάνω import επέστρεψε ModuleNotFoundError, που σημαίνει ότι δεν βρέθηκε το πακέτο newspaper. Άρα πρέπει πρώτα να το εγκαταστήσουμε.\n",
        "\n",
        "Εκτελούμε την παρακάτω εντολή εγκατάστασης και **αφού εγκατασταθεί το μετατρέπουμε σε σχόλιο** (βάζοντας στην αρχή της εντολής #)."
      ],
      "metadata": {
        "id": "GlvjGeCIxcec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install newspaper3k"
      ],
      "metadata": {
        "id": "60VAlormhbOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τώρα μπορούμε να εισάγουμε το newspaper."
      ],
      "metadata": {
        "id": "TTICKEfSzGvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Χρήση του newspaper και συλλογή άρθρων**"
      ],
      "metadata": {
        "id": "c5oN2ONbwp1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTMHjL80hGJV"
      },
      "outputs": [],
      "source": [
        "import newspaper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Όπως είπαμε παραπάνω, στην περιγραφή των βημάτων για το scraping, το πρώτο πράγμα που πρέπει να κάνουμε είναι να ορίσουμε το url της ειδησεογραφικής σελίδας από την οποία θα αντλήσουμε τα περιεχόμενα. \n",
        "\n",
        "Εδώ, θα συλλέξουμε άρθρα από την κατηγορία news του bbc."
      ],
      "metadata": {
        "id": "mgoh19vxzV30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site_url = \"https://www.bbc.com/news\""
      ],
      "metadata": {
        "id": "tlJJGd8MzTDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the content of the site\n",
        "site = newspaper.build(site_url, memoize_articles=False)"
      ],
      "metadata": {
        "id": "bgxehmmL0NpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get list of article URLs\n",
        "urlsL = site.article_urls()\n",
        "\n",
        "print(len(urlsL))\n",
        "# print the first 10 articles urls\n",
        "urlsL[:10]"
      ],
      "metadata": {
        "id": "6QJ60uM20bC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Έχουμε λοιπόν urls από το bbc.com/news.\n",
        "\n",
        "Τώρα, ας φέρουμε κάποια από τα άρθρα που υπάρχουν σε αυτά τα urls."
      ],
      "metadata": {
        "id": "skUSivSC1cPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y8xzCPcHljfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για αρχή, ας φέρουμε το περιεχόμενο μόνο ενός άρθρου από το πρώτο url"
      ],
      "metadata": {
        "id": "0jjEBiUF24En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = Article(urlsL[0])\n",
        "article.download()\n",
        "article.parse()"
      ],
      "metadata": {
        "id": "Eic_8Kkh3TDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Με τις παραπάνω εντολές, έχουμε κατεβάσει το πρώτο άρθρο και τώρα μπορούμε να δούμε το περιεχόμενό του. Ας δούμε τι έχουμε:"
      ],
      "metadata": {
        "id": "cEMxKOOl32jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article.title"
      ],
      "metadata": {
        "id": "weBa_Gms4HdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.text"
      ],
      "metadata": {
        "id": "cxqSXG9Q4MWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.authors"
      ],
      "metadata": {
        "id": "SRFrQArf4P6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.top_image"
      ],
      "metadata": {
        "id": "fjkBZ0MitCn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.images"
      ],
      "metadata": {
        "id": "aw3unUwy4UKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.movies"
      ],
      "metadata": {
        "id": "LNd6O8rz4YBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.publish_date"
      ],
      "metadata": {
        "id": "N6VWW4gkrchv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αφού είδαμε ποιες πληροφορίες μπορούμε να πάρουμε για ένα url, ας φέρουμε αυτές τις πληροφορίες για τα 10 πρώτα urls κι ας τις οργανώσουμε σε ένα dataframe."
      ],
      "metadata": {
        "id": "t2OaEhP55Hnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articlesL = []\n",
        "for url in urlsL[:10]:\n",
        "  articleD = {}\n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  articleD['url'] = url\n",
        "  if article.publish_date is not None:\n",
        "    articleD['date'] = article.publish_date\n",
        "  else:\n",
        "    articleD['date'] = np.nan\n",
        "  if article.title is not None:\n",
        "    articleD['title'] = article.title\n",
        "  else:\n",
        "    articleD['title'] = np.nan\n",
        "  if article.top_image is not None:\n",
        "    articleD['top image'] = article.top_image\n",
        "  else:\n",
        "    articleD['top image'] = np.nan\n",
        "  if article.text is not None:\n",
        "    articleD['text'] = article.text\n",
        "  else:\n",
        "    articleD['text'] = np.nan\n",
        "  if article.authors is not None:\n",
        "    articleD['authors'] = article.authors\n",
        "  else:\n",
        "    articleD['authors'] = np.nan\n",
        "  if article.images is not None:\n",
        "    articleD['images'] = article.images\n",
        "  else:\n",
        "    articleD['images'] = np.nan\n",
        "  if article.movies is not None:\n",
        "    articleD['movies'] = article.movies\n",
        "  else:\n",
        "    articleD['movies'] = np.nan\n",
        "  articlesL.append(articleD)\n",
        "\n",
        "news_df = pd.DataFrame(articlesL)"
      ],
      "metadata": {
        "id": "3FrK7B2QhxR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df"
      ],
      "metadata": {
        "id": "eFme0Bt2l2XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ολόκληρος ο κώδικας για να φέρουμε το περιεχόμενο των άρθρων από ένα site"
      ],
      "metadata": {
        "id": "kQkQf1jPn0Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define site url\n",
        "site_url = \"https://www.bbc.com/news\"\n",
        "# get the content of the site\n",
        "site = newspaper.build(site_url, memoize_articles=False)\n",
        "# get list of article URLs\n",
        "urlsL = site.article_urls()\n",
        "\n",
        "# get articles content (first 10)\n",
        "articlesL = []\n",
        "for url in urlsL[:10]:\n",
        "  articleD = {}\n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  articleD['url'] = url\n",
        "  if article.publish_date is not None:\n",
        "    articleD['date'] = article.publish_date\n",
        "  else:\n",
        "    articleD['date'] = np.nan\n",
        "  if article.title is not None:\n",
        "    articleD['title'] = article.title\n",
        "  else:\n",
        "    articleD['title'] = np.nan\n",
        "  if article.top_image is not None:\n",
        "    articleD['top image'] = article.top_image\n",
        "  else:\n",
        "    articleD['top image'] = np.nan\n",
        "  if article.text is not None:\n",
        "    articleD['text'] = article.text\n",
        "  else:\n",
        "    articleD['text'] = np.nan\n",
        "  if article.authors is not None:\n",
        "    articleD['authors'] = article.authors\n",
        "  else:\n",
        "    articleD['authors'] = np.nan\n",
        "  if article.images is not None:\n",
        "    articleD['images'] = article.images\n",
        "  else:\n",
        "    articleD['images'] = np.nan\n",
        "  if article.movies is not None:\n",
        "    articleD['movies'] = article.movies\n",
        "  else:\n",
        "    articleD['movies'] = np.nan\n",
        "  articlesL.append(articleD)\n",
        "\n",
        "news_df = pd.DataFrame(articlesL)"
      ],
      "metadata": {
        "id": "JOUZXyThnynY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Άσκηση:** χρησιμοποιήστε τον παραπάνω κώδικα για να φτιάξετε ένα dataframe με άρθρα από το news247 και την efsyn."
      ],
      "metadata": {
        "id": "jgii90WQoFZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**news247**"
      ],
      "metadata": {
        "id": "clpJWDaHxw-l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8Y0KDndIbmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**efsyn**"
      ],
      "metadata": {
        "id": "8kOGVDD5xfYs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3NlHyG6IeKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}